knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # Loading the "tidyverse" library for data wrangling, analysis, and visualization...
library(deeplr) #
debate <- read_csv("Debate 2009.csv", show_col_types = FALSE)
responders <- read_csv("Responders2009.csv", show_col_types = FALSE)
content_analysis <- read.csv("Content analysis debate 2009.csv")
responders <- rename(responders,
Location = Standort, Participation_W1 = Teilnahme1,
Participation_W2 = Teilnahme2,
Participation_W3 = Teilnahme3,
Participation_W4 = Teilnahme4
)
debate <- debate |>
pivot_longer(cols = starts_with("t"),
names_to = "time_str",
values_to = "score") |>
mutate(time_str = str_remove_all(time_str, "^t"),
time_str = str_replace(time_str, "(..)(..)(..)", "\\1:\\2:\\3"),
time = as.duration(hms(time_str))
)
responders <- responders |>
rename_with(~sub("(^[A-D])(.*)", "\\1-\\2", .), matches("^[A-D]")) |>
pivot_longer(
cols = starts_with("A") | starts_with("B") | starts_with("C") | starts_with("D"),
names_to = c("Wave", ".value"),
names_sep = "-"
) |>
relocate(Responder, Wave)
View(responders)
View(responders)
responders |> count(Responder, Wave) |> filter(n > 1)
debate |> count(Responder, time) |> filter(n > 1)
content_analysis |> count(Fortlaufende) |> filter(n > 1)
knitr::opts_chunk$set(echo = TRUE)
library(usethis)
# If needed, the user must install the "deeplr" library, and the Github "deeplr" repository via the "devtools" package.
# install.packages("deeplr", source = TRUE)
# install.packages("devtools")
# devtools::install_github("zumbov2/deeplr")
library(tidyverse) # Loading the "tidyverse" library for data wrangling, analysis, and visualization...
library(deeplr) # ...and the "deeplr" library, for translating textual data in German with DeepL.
library(usethis) # ???
# If needed, the user must install the "deeplr" library, and the Github "deeplr" repository via the "devtools" package.
# install.packages("deeplr", source = TRUE)
# install.packages("devtools")
# devtools::install_github("zumbov2/deeplr")
library(tidyverse) # Loading the "tidyverse" library for data wrangling, analysis, and visualization...
library(deeplr) # ...and the "deeplr" library, for translating textual data in German with DeepL.
library(usethis) # ???
use_course("ccmarzer/Group-Project")
debate <- read_csv(url("https://github.com/ccmarzer/Group-Project/blob/main/content_analysis_eng.csv"), show_col_types = FALSE) # Importing the "Debate 2009.csv" data, containing the real-time reactions of all respondents during the debate.
debate <- read_csv(url("https://raw.githubusercontent.com/ccmarzer/Group-Project/main/content_analysis_eng.csv?token=GHSAT0AAAAAACMWXOEM4RK5LMGV2NVATV4SZN6NJHA"), show_col_types = FALSE) # Importing the "Debate 2009.csv" data, containing the real-time reactions of all respondents during the debate.
responders <- read_csv("https://raw.githubusercontent.com/ccmarzer/Group-Project/main/Responders2009.csv?token=GHSAT0AAAAAACMWXOENF2BSICNHWBQFB4N4ZN6NKDQ", show_col_types = FALSE) # Importing the "Responders2009.csv" data, containing panel data concerning socio-demographic information, and political opinions and behaviours, of all respondents.
content_analysis <- read.csv("https://raw.githubusercontent.com/ccmarzer/Group-Project/main/Responders2009.csv?token=GHSAT0AAAAAACMWXOENWECA5SKQ75B54WDSZN6NLOA") # Importing the version of the "Content analysis debate 2009.csv" data translated in English, containing information regarding the content analysis of the TV debate of 2009.
content_analysis <- read.csv("https://raw.githubusercontent.com/ccmarzer/Group-Project/main/content_analysis_eng.csv?token=GHSAT0AAAAAACMWXOEM4RK5LMGV2NVATV4SZN6NJHA") # Importing the version of the "Content analysis debate 2009.csv" data translated in English, containing information regarding the content analysis of the TV debate of 2009.
debate <- read_csv(url("https://raw.githubusercontent.com/ccmarzer/Group-Project/main/Debate%202009.csv?token=GHSAT0AAAAAACMWXOEMOYXEBKFQ6OZIGMUOZN6NMDA"), show_col_types = FALSE) # Importing the "Debate 2009.csv" data, containing the real-time reactions of all respondents during the debate.
